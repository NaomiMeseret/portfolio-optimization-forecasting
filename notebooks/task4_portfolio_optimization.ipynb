{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a01a3d82",
      "metadata": {},
      "source": [
        "# Task 4: Optimize Portfolio Based on Forecast\n",
        "\n",
        "## Objective\n",
        "Use the TSLA return forecast from Task 3 together with historical returns for BND and SPY to construct an optimal TSLA/BND/SPY portfolio using Modern Portfolio Theory (MPT).\n",
        "\n",
        "> Prerequisite: Run Task 1–3 first so that processed data and TSLA forecasts exist in `../data/processed/`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a297be7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully for Task 4.\n"
          ]
        }
      ],
      "source": [
        "# 1. Imports and configuration\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully for Task 4.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a968048c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Historical daily returns shape: (2774, 3)\n",
            "Model comparison CSV not found; inferring best model from cached artifacts...\n",
            "Found SARIMA model artifact. Selecting 'SARIMA'.\n",
            "Best model from Task 2: SARIMA\n"
          ]
        }
      ],
      "source": [
        "# 2. Load processed data and choose TSLA forecast source\n",
        "\n",
        "# Load processed data created in Task 1\n",
        "tsla_data = pd.read_csv('../data/processed/TSLA_processed.csv', index_col='Date', parse_dates=True)\n",
        "bnd_data  = pd.read_csv('../data/processed/BND_processed.csv',  index_col='Date', parse_dates=True)\n",
        "spy_data  = pd.read_csv('../data/processed/SPY_processed.csv',  index_col='Date', parse_dates=True)\n",
        "\n",
        "# Ensure daily returns exist (recompute to be safe)\n",
        "for df in [tsla_data, bnd_data, spy_data]:\n",
        "    if 'Daily_Return' not in df.columns:\n",
        "        # Use Adj Close if available, otherwise Close\n",
        "        price_col = 'Adj Close' if 'Adj Close' in df.columns else 'Close'\n",
        "        df['Daily_Return'] = df[price_col].pct_change()\n",
        "\n",
        "# Historical daily returns for all three assets\n",
        "returns_hist = pd.DataFrame({\n",
        "    'TSLA': tsla_data['Daily_Return'],\n",
        "    'BND':  bnd_data['Daily_Return'],\n",
        "    'SPY':  spy_data['Daily_Return']\n",
        "}).dropna()\n",
        "\n",
        "print(\"Historical daily returns shape:\", returns_hist.shape)\n",
        "\n",
        "# Load model comparison results (to know which model was best). If missing, infer from cached artifacts\n",
        "comparison_csv = '../data/processed/model_comparison_results.csv'\n",
        "best_model_name = None\n",
        "if os.path.exists(comparison_csv):\n",
        "    comparison_df = pd.read_csv(comparison_csv)\n",
        "    best_row = comparison_df.loc[comparison_df['RMSE'].idxmin()]\n",
        "    best_model_name = best_row['Model']\n",
        "else:\n",
        "    print(\"Model comparison CSV not found; inferring best model from cached artifacts...\")\n",
        "    sarima_path = '../data/processed/sarima_model.pkl'\n",
        "    lstm_path = '../data/processed/lstm_model.h5'\n",
        "    if os.path.exists(sarima_path) and os.path.exists(lstm_path):\n",
        "        print(\"Both SARIMA and LSTM artifacts found. Defaulting to 'SARIMA'.\")\n",
        "        best_model_name = 'SARIMA'\n",
        "    elif os.path.exists(sarima_path):\n",
        "        print(\"Found SARIMA model artifact. Selecting 'SARIMA'.\")\n",
        "        best_model_name = 'SARIMA'\n",
        "    elif os.path.exists(lstm_path):\n",
        "        print(\"Found LSTM model artifact. Selecting 'LSTM'.\")\n",
        "        best_model_name = 'LSTM'\n",
        "    else:\n",
        "        print(\"No model artifacts found. Defaulting to 'SARIMA'.\")\n",
        "        best_model_name = 'SARIMA'\n",
        "\n",
        "print(\"Best model from Task 2:\", best_model_name)\n",
        "\n",
        "# Load TSLA future forecasts saved from Task 3 (if you choose to save them there later).\n",
        "# For now, we will assume we regenerate them here in Task 4 (simpler workflow)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "029ccb86",
      "metadata": {},
      "source": [
        "## 3. Compute Expected Returns and Covariance Matrix\n",
        "\n",
        "- **TSLA (Forecasted Asset)**: Use the forecasted price path to derive expected daily and annual returns.\n",
        "- **BND & SPY (Historical Assets)**: Use historical average daily returns, annualized.\n",
        "- Compute the **annualized covariance matrix** of daily returns for all three assets, which will be used for portfolio risk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31c5ab8e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.1 Compute expected annual returns\n",
        "from datetime import timedelta\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "import pmdarima as pm\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "trading_days = 252\n",
        "\n",
        "# Reuse Adjusted Close for TSLA (fallback to Close if Adj Close not available)\n",
        "if 'Adj Close' in tsla_data.columns:\n",
        "    tsla_close = tsla_data['Adj Close'].dropna()\n",
        "else:\n",
        "    tsla_close = tsla_data['Close'].dropna()\n",
        "\n",
        "# Recreate train/test split to stay consistent with Task 2\n",
        "split_date = pd.to_datetime('2025-01-01')\n",
        "\n",
        "# Ensure split_date matches the timezone of the index\n",
        "if hasattr(tsla_close.index, 'tz') and tsla_close.index.tz is not None:\n",
        "    # Index is timezone-aware, make split_date timezone-aware too\n",
        "    split_date = split_date.tz_localize(tsla_close.index.tz)\n",
        "# If index is timezone-naive, split_date is already naive, so no change needed\n",
        "\n",
        "train_data = tsla_close[tsla_close.index < split_date]\n",
        "\n",
        "forecast_steps = 252  # ~12 months horizon\n",
        "\n",
        "future_prices = None\n",
        "\n",
        "if best_model_name == 'SARIMA':\n",
        "    print(\"Using SARIMA-based forecast for TSLA expected return...\")\n",
        "\n",
        "    auto_arima_model = pm.auto_arima(\n",
        "        tsla_close,\n",
        "        start_p=0, start_q=0,\n",
        "        max_p=5, max_q=5,\n",
        "        start_P=0, start_Q=0,\n",
        "        max_P=2, max_Q=2,\n",
        "        m=12,\n",
        "        seasonal=True,\n",
        "        stepwise=True,\n",
        "        suppress_warnings=True,\n",
        "        error_action='ignore',\n",
        "        trace=False\n",
        "    )\n",
        "\n",
        "    arima_order = auto_arima_model.order\n",
        "    seasonal_order = auto_arima_model.seasonal_order\n",
        "\n",
        "    sarima_model = SARIMAX(\n",
        "        tsla_close,\n",
        "        order=arima_order,\n",
        "        seasonal_order=seasonal_order,\n",
        "        enforce_stationarity=False,\n",
        "        enforce_invertibility=False\n",
        "    )\n",
        "\n",
        "    fitted_sarima = sarima_model.fit(disp=False)\n",
        "    sarima_future = fitted_sarima.get_forecast(steps=forecast_steps)\n",
        "    future_prices = sarima_future.predicted_mean.values\n",
        "\n",
        "elif best_model_name == 'LSTM':\n",
        "    print(\"Using LSTM-based forecast for TSLA expected return...\")\n",
        "\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    tsla_scaled = scaler.fit_transform(tsla_close.values.reshape(-1, 1))\n",
        "    train_scaled = scaler.transform(train_data.values.reshape(-1, 1))\n",
        "\n",
        "    seq_length = 60\n",
        "\n",
        "    def create_sequences(data, seq_len=60):\n",
        "        X, y = [], []\n",
        "        for i in range(seq_len, len(data)):\n",
        "            X.append(data[i-seq_len:i, 0])\n",
        "            y.append(data[i, 0])\n",
        "        return np.array(X), np.array(y)\n",
        "\n",
        "    X_train, y_train = create_sequences(train_scaled, seq_length)\n",
        "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "\n",
        "    lstm_model = Sequential([\n",
        "        LSTM(50, return_sequences=True, input_shape=(seq_length, 1)),\n",
        "        Dropout(0.2),\n",
        "        LSTM(50, return_sequences=True),\n",
        "        Dropout(0.2),\n",
        "        LSTM(50),\n",
        "        Dropout(0.2),\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    lstm_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "    history = lstm_model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=50,\n",
        "        batch_size=32,\n",
        "        validation_split=0.2,\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    last_seq = tsla_scaled[-seq_length:].reshape(1, seq_length, 1)\n",
        "    future_scaled = []\n",
        "\n",
        "    for _ in range(forecast_steps):\n",
        "        next_pred = lstm_model.predict(last_seq, verbose=0)[0, 0]\n",
        "        future_scaled.append(next_pred)\n",
        "        last_seq = np.append(last_seq[:, 1:, :], [[[next_pred]]], axis=1)\n",
        "\n",
        "    future_scaled = np.array(future_scaled).reshape(-1, 1)\n",
        "    future_prices = scaler.inverse_transform(future_scaled).flatten()\n",
        "\n",
        "# Convert forecast prices to daily forecast returns\n",
        "future_returns_tsla = pd.Series(future_prices).pct_change().dropna()\n",
        "mu_tsla_forecast_daily = future_returns_tsla.mean()\n",
        "mu_tsla_forecast_annual = mu_tsla_forecast_daily * trading_days\n",
        "\n",
        "# Historical expected returns for BND and SPY\n",
        "mu_bnd_annual = returns_hist['BND'].mean() * trading_days\n",
        "mu_spy_annual = returns_hist['SPY'].mean() * trading_days\n",
        "\n",
        "print(\"\\nApproximate expected annual returns:\")\n",
        "print(f\"TSLA (from forecast): {mu_tsla_forecast_annual:.4f}\")\n",
        "print(f\"BND  (historical):   {mu_bnd_annual:.4f}\")\n",
        "print(f\"SPY  (historical):   {mu_spy_annual:.4f}\")\n",
        "\n",
        "mu_vector = np.array([mu_tsla_forecast_annual, mu_bnd_annual, mu_spy_annual])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85ea35c9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.2 Annualized covariance matrix of daily returns\n",
        "\n",
        "cov_daily = returns_hist.cov()\n",
        "cov_annual = cov_daily * trading_days\n",
        "\n",
        "print(\"Annualized covariance matrix:\")\n",
        "display(cov_annual)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce36f8fd",
      "metadata": {},
      "source": [
        "## 4. Generate Efficient Frontier and Key Portfolios\n",
        "\n",
        "We now use **PyPortfolioOpt** to:\n",
        "- Build the efficient frontier from the expected return vector and annualized covariance matrix.\n",
        "- Identify the **Maximum Sharpe Ratio (tangency) portfolio**.\n",
        "- Identify the **Minimum Volatility portfolio**.\n",
        "- Visualize the efficient frontier with these portfolios highlighted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4560ff1b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4.1 Efficient Frontier, Max Sharpe, and Min Volatility Portfolios\n",
        "\n",
        "from pypfopt.efficient_frontier import EfficientFrontier\n",
        "from pypfopt import plotting\n",
        "\n",
        "assets = ['TSLA', 'BND', 'SPY']\n",
        "\n",
        "# Build efficient frontier\n",
        "ef = EfficientFrontier(mu_vector, cov_annual)\n",
        "\n",
        "# Maximum Sharpe ratio portfolio\n",
        "max_sharpe_weights = ef.max_sharpe(risk_free_rate=0.02)\n",
        "cleaned_max_sharpe = ef.clean_weights()\n",
        "max_sharpe_perf = ef.portfolio_performance(verbose=True)\n",
        "\n",
        "print(\"\\nMax Sharpe Ratio Portfolio Weights:\")\n",
        "for asset, w in cleaned_max_sharpe.items():\n",
        "    print(f\"  {asset}: {w:.4f}\")\n",
        "\n",
        "# Minimum volatility portfolio\n",
        "ef_minvol = EfficientFrontier(mu_vector, cov_annual)\n",
        "min_vol_weights = ef_minvol.min_volatility()\n",
        "cleaned_min_vol = ef_minvol.clean_weights()\n",
        "min_vol_perf = ef_minvol.portfolio_performance(verbose=True)\n",
        "\n",
        "print(\"\\nMinimum Volatility Portfolio Weights:\")\n",
        "for asset, w in cleaned_min_vol.items():\n",
        "    print(f\"  {asset}: {w:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09de03dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4.2 Visualize Efficient Frontier with Key Portfolios\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "plotting.plot_efficient_frontier(ef_minvol, ax=ax, show_assets=False)\n",
        "\n",
        "# Plot the two key portfolios\n",
        "ax.scatter(min_vol_perf[1], min_vol_perf[0], marker='o', s=80, color='green', label='Min Volatility')\n",
        "ax.scatter(max_sharpe_perf[1], max_sharpe_perf[0], marker='*', s=200, color='red', label='Max Sharpe')\n",
        "\n",
        "ax.set_title('Efficient Frontier – TSLA/BND/SPY', fontsize=14, fontweight='bold')\n",
        "ax.set_xlabel('Volatility (Risk)')\n",
        "ax.set_ylabel('Expected Return')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('../data/processed/task4_efficient_frontier.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
