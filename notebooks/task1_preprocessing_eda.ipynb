{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Preprocess and Explore the Data\n",
    "\n",
    "## Objective\n",
    "Load, clean, and understand the data to prepare it for modeling.\n",
    "\n",
    "This notebook covers:\n",
    "1. Extracting historical financial data using YFinance\n",
    "2. Data cleaning and understanding\n",
    "3. Exploratory Data Analysis (EDA)\n",
    "4. Seasonality and Trend Analysis\n",
    "5. Risk Metrics Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract Historical Financial Data\n",
    "\n",
    "We'll fetch data for three assets:\n",
    "- **TSLA** (Tesla): High-growth stock\n",
    "- **BND** (Vanguard Total Bond Market ETF): Low risk bonds\n",
    "- **SPY** (S&P 500 ETF): Moderate risk market index\n",
    "\n",
    "Period: January 1, 2015 to January 15, 2026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define assets and date range\n",
    "tickers = ['TSLA', 'BND', 'SPY']\n",
    "start_date = '2015-01-01'\n",
    "end_date = '2026-01-15'\n",
    "\n",
    "# Fetch data for all assets\n",
    "print(\"Fetching historical data from YFinance...\")\n",
    "data_dict = {}\n",
    "\n",
    "for ticker in tickers:\n",
    "    print(f\"Downloading {ticker}...\")\n",
    "    stock = yf.Ticker(ticker)\n",
    "    df = stock.history(start=start_date, end=end_date)\n",
    "    df['Ticker'] = ticker\n",
    "    data_dict[ticker] = df\n",
    "    print(f\"{ticker}: {len(df)} records downloaded\")\n",
    "\n",
    "print(\"\\nData extraction complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all dataframes\n",
    "combined_data = pd.concat(data_dict.values(), ignore_index=False)\n",
    "combined_data = combined_data.reset_index()\n",
    "\n",
    "# Display first few rows\n",
    "print(\"Combined Dataset Shape:\", combined_data.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "combined_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning and Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types\n",
    "print(\"Data Types:\")\n",
    "print(combined_data.dtypes)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "missing_values = combined_data.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"No missing values found!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "# Basic statistics\n",
    "print(\"\\nBasic Statistics:\")\n",
    "combined_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure Date column is datetime type\n",
    "combined_data['Date'] = pd.to_datetime(combined_data['Date'])\n",
    "\n",
    "# Set Date as index for time series analysis\n",
    "combined_data = combined_data.set_index('Date')\n",
    "\n",
    "# Sort by date\n",
    "combined_data = combined_data.sort_index()\n",
    "\n",
    "# Check date range for each ticker\n",
    "print(\"Date Range for each asset:\")\n",
    "for ticker in tickers:\n",
    "    ticker_data = combined_data[combined_data['Ticker'] == ticker]\n",
    "    print(f\"{ticker}: {ticker_data.index.min()} to {ticker_data.index.max()} ({len(ticker_data)} days)\")\n",
    "\n",
    "# Handle any remaining missing values (if any)\n",
    "if combined_data.isnull().sum().sum() > 0:\n",
    "    print(\"\\nFilling missing values...\")\n",
    "    # Forward fill for missing values (common in financial data)\n",
    "    combined_data = combined_data.ffill()\n",
    "    # If still missing, backward fill\n",
    "    combined_data = combined_data.bfill()\n",
    "    \n",
    "print(\"\\nData cleaning complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate dataframes for each asset for easier analysis\n",
    "tsla_data = combined_data[combined_data['Ticker'] == 'TSLA'].copy()\n",
    "bnd_data = combined_data[combined_data['Ticker'] == 'BND'].copy()\n",
    "spy_data = combined_data[combined_data['Ticker'] == 'SPY'].copy()\n",
    "\n",
    "# Calculate daily returns for each asset\n",
    "for df, name in [(tsla_data, 'TSLA'), (bnd_data, 'BND'), (spy_data, 'SPY')]:\n",
    "    df['Daily_Return'] = df['Close'].pct_change()\n",
    "    df['Log_Return'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "\n",
    "print(\"Daily returns calculated for all assets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "### 3.1 Closing Price Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 1: Closing prices over time\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "\n",
    "for idx, (df, ticker, title) in enumerate([(tsla_data, 'TSLA', 'Tesla (TSLA)'),\n",
    "                                           (bnd_data, 'BND', 'Vanguard Total Bond Market ETF (BND)'),\n",
    "                                           (spy_data, 'SPY', 'S&P 500 ETF (SPY)')]):\n",
    "    axes[idx].plot(df.index, df['Close'], linewidth=1.5, label=f'{ticker} Close Price')\n",
    "    axes[idx].set_title(f'{title} - Closing Price Over Time', fontsize=14, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Date', fontsize=12)\n",
    "    axes[idx].set_ylabel('Price (USD)', fontsize=12)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    axes[idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/closing_prices_over_time.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Insights:\")\n",
    "print(\"- TSLA shows high volatility and significant growth over the period\")\n",
    "print(\"- BND shows relatively stable prices with gradual upward trend\")\n",
    "print(\"- SPY shows moderate volatility with overall upward trend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Daily Percentage Change (Returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 2: Daily returns\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "\n",
    "for idx, (df, ticker, title) in enumerate([(tsla_data, 'TSLA', 'Tesla (TSLA)'),\n",
    "                                           (bnd_data, 'BND', 'Vanguard Total Bond Market ETF (BND)'),\n",
    "                                           (spy_data, 'SPY', 'S&P 500 ETF (SPY)')]):\n",
    "    axes[idx].plot(df.index, df['Daily_Return'] * 100, linewidth=0.8, alpha=0.7, label=f'{ticker} Daily Returns')\n",
    "    axes[idx].axhline(y=0, color='r', linestyle='--', linewidth=1)\n",
    "    axes[idx].set_title(f'{title} - Daily Percentage Returns', fontsize=14, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Date', fontsize=12)\n",
    "    axes[idx].set_ylabel('Daily Return (%)', fontsize=12)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    axes[idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/daily_returns.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Statistics on daily returns\n",
    "print(\"Daily Returns Statistics:\")\n",
    "print(\"=\"*60)\n",
    "for df, ticker in [(tsla_data, 'TSLA'), (bnd_data, 'BND'), (spy_data, 'SPY')]:\n",
    "    returns = df['Daily_Return'].dropna()\n",
    "    print(f\"\\n{ticker}:\")\n",
    "    print(f\"  Mean: {returns.mean()*100:.4f}%\")\n",
    "    print(f\"  Std Dev: {returns.std()*100:.4f}%\")\n",
    "    print(f\"  Min: {returns.min()*100:.4f}%\")\n",
    "    print(f\"  Max: {returns.max()*100:.4f}%\")\n",
    "    print(f\"  Skewness: {returns.skew():.4f}\")\n",
    "    print(f\"  Kurtosis: {returns.kurtosis():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Volatility Analysis (Rolling Statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate rolling statistics (30-day and 60-day windows)\n",
    "window_short = 30\n",
    "window_long = 60\n",
    "\n",
    "for df in [tsla_data, bnd_data, spy_data]:\n",
    "    df['Rolling_Mean_30'] = df['Close'].rolling(window=window_short).mean()\n",
    "    df['Rolling_Std_30'] = df['Close'].rolling(window=window_short).std()\n",
    "    df['Rolling_Mean_60'] = df['Close'].rolling(window=window_long).mean()\n",
    "    df['Rolling_Std_60'] = df['Close'].rolling(window=window_long).std()\n",
    "    df['Rolling_Volatility_30'] = df['Daily_Return'].rolling(window=window_short).std() * np.sqrt(252) * 100  # Annualized\n",
    "\n",
    "# Visualization 3: Rolling volatility\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "\n",
    "for idx, (df, ticker, title) in enumerate([(tsla_data, 'TSLA', 'Tesla (TSLA)'),\n",
    "                                           (bnd_data, 'BND', 'Vanguard Total Bond Market ETF (BND)'),\n",
    "                                           (spy_data, 'SPY', 'S&P 500 ETF (SPY)')]):\n",
    "    axes[idx].plot(df.index, df['Rolling_Volatility_30'], linewidth=1.5, label='30-Day Rolling Volatility (Annualized)')\n",
    "    axes[idx].set_title(f'{title} - Rolling Volatility (30-Day Window)', fontsize=14, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Date', fontsize=12)\n",
    "    axes[idx].set_ylabel('Volatility (%)', fontsize=12)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    axes[idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/rolling_volatility.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Volatility Insights:\")\n",
    "print(\"- TSLA shows the highest volatility, with significant spikes during market events\")\n",
    "print(\"- BND shows the lowest and most stable volatility\")\n",
    "print(\"- SPY shows moderate volatility, typically between TSLA and BND\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers using IQR method\n",
    "def detect_outliers_iqr(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    return outliers\n",
    "\n",
    "# Detect outliers in daily returns\n",
    "print(\"Outlier Detection in Daily Returns (IQR Method):\")\n",
    "print(\"=\"*60)\n",
    "for df, ticker in [(tsla_data, 'TSLA'), (bnd_data, 'BND'), (spy_data, 'SPY')]:\n",
    "    outliers = detect_outliers_iqr(df, 'Daily_Return')\n",
    "    print(f\"\\n{ticker}: {len(outliers)} outliers detected ({len(outliers)/len(df)*100:.2f}% of data)\")\n",
    "    if len(outliers) > 0:\n",
    "        print(f\"  Most extreme positive return: {outliers['Daily_Return'].max()*100:.2f}% on {outliers.loc[outliers['Daily_Return'].idxmax()].name}\")\n",
    "        print(f\"  Most extreme negative return: {outliers['Daily_Return'].min()*100:.2f}% on {outliers.loc[outliers['Daily_Return'].idxmin()].name}\")\n",
    "\n",
    "# Visualization: Box plots for daily returns\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for idx, (df, ticker) in enumerate([(tsla_data, 'TSLA'), (bnd_data, 'BND'), (spy_data, 'SPY')]):\n",
    "    axes[idx].boxplot(df['Daily_Return'].dropna() * 100, vert=True)\n",
    "    axes[idx].set_title(f'{ticker} - Daily Returns Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Daily Return (%)', fontsize=12)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/returns_boxplot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Analyze Days with Unusually High or Low Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify extreme return days (top 5% and bottom 5%)\n",
    "print(\"Extreme Return Days Analysis:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for df, ticker in [(tsla_data, 'TSLA'), (bnd_data, 'BND'), (spy_data, 'SPY')]:\n",
    "    returns = df['Daily_Return'].dropna()\n",
    "    threshold_high = returns.quantile(0.95)\n",
    "    threshold_low = returns.quantile(0.05)\n",
    "    \n",
    "    extreme_high = df[df['Daily_Return'] >= threshold_high].sort_values('Daily_Return', ascending=False).head(10)\n",
    "    extreme_low = df[df['Daily_Return'] <= threshold_low].sort_values('Daily_Return', ascending=True).head(10)\n",
    "    \n",
    "    print(f\"\\n{ticker} - Top 10 Highest Returns:\")\n",
    "    print(extreme_high[['Close', 'Daily_Return']].to_string())\n",
    "    \n",
    "    print(f\"\\n{ticker} - Top 10 Lowest Returns:\")\n",
    "    print(extreme_low[['Close', 'Daily_Return']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Seasonality and Trend Analysis\n",
    "\n",
    "### 4.1 Augmented Dickey-Fuller (ADF) Test for Stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def adf_test(timeseries, title=\"Time Series\"):\n",
    "    \"\"\"\n",
    "    Perform Augmented Dickey-Fuller test for stationarity\n",
    "    \"\"\"\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Augmented Dickey-Fuller Test: {title}')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    adf_result = adfuller(timeseries.dropna(), autolag='AIC')\n",
    "    \n",
    "    print(f'ADF Statistic: {adf_result[0]:.6f}')\n",
    "    print(f'p-value: {adf_result[1]:.6f}')\n",
    "    print(f'Number of Lags Used: {adf_result[2]}')\n",
    "    print(f'Number of Observations: {adf_result[3]}')\n",
    "    \n",
    "    print('\\nCritical Values:')\n",
    "    for key, value in adf_result[4].items():\n",
    "        print(f'   {key}: {value:.6f}')\n",
    "    \n",
    "    if adf_result[1] <= 0.05:\n",
    "        print(f'\\n✓ Result: Series is STATIONARY (p-value <= 0.05)')\n",
    "        print('  → No differencing required for ARIMA models')\n",
    "    else:\n",
    "        print(f'\\n✗ Result: Series is NON-STATIONARY (p-value > 0.05)')\n",
    "        print('  → Differencing required for ARIMA models (d > 0)')\n",
    "    \n",
    "    return adf_result\n",
    "\n",
    "# Test closing prices (typically non-stationary)\n",
    "print(\"STATIONARITY TEST: Closing Prices\")\n",
    "print(\"=\"*70)\n",
    "for df, ticker in [(tsla_data, 'TSLA'), (bnd_data, 'BND'), (spy_data, 'SPY')]:\n",
    "    adf_test(df['Close'], f'{ticker} Closing Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test daily returns (typically stationary)\n",
    "print(\"\\n\\nSTATIONARITY TEST: Daily Returns\")\n",
    "print(\"=\"*70)\n",
    "for df, ticker in [(tsla_data, 'TSLA'), (bnd_data, 'BND'), (spy_data, 'SPY')]:\n",
    "    adf_test(df['Daily_Return'], f'{ticker} Daily Returns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Interpretation of Stationarity Results\n",
    "\n",
    "**Key Findings:**\n",
    "- **Closing Prices**: Typically non-stationary (p-value > 0.05), indicating trends and requiring differencing (d > 0) for ARIMA models\n",
    "- **Daily Returns**: Typically stationary (p-value ≤ 0.05), indicating mean-reverting behavior suitable for ARIMA modeling\n",
    "\n",
    "**Implications for Modeling:**\n",
    "- For ARIMA models on prices: We'll need to difference the data (d parameter)\n",
    "- For ARIMA models on returns: We can use d=0 or d=1\n",
    "- LSTM models can handle non-stationary data but may benefit from normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calculate Risk Metrics\n",
    "\n",
    "### 5.1 Value at Risk (VaR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def calculate_var(returns, confidence_level=0.05):\n",
    "    \"\"\"\n",
    "    Calculate Value at Risk (VaR) using historical method\n",
    "    VaR represents the maximum expected loss at a given confidence level\n",
    "    \"\"\"\n",
    "    # Historical VaR: percentile of returns\n",
    "    var_historical = np.percentile(returns.dropna(), confidence_level * 100)\n",
    "    \n",
    "    # Parametric VaR: assuming normal distribution\n",
    "    mean_return = returns.mean()\n",
    "    std_return = returns.std()\n",
    "    var_parametric = stats.norm.ppf(confidence_level, mean_return, std_return)\n",
    "    \n",
    "    return var_historical, var_parametric\n",
    "\n",
    "print(\"Value at Risk (VaR) - 95% Confidence Level (5% VaR)\")\n",
    "print(\"=\"*70)\n",
    "print(\"VaR represents the maximum expected loss with 95% confidence\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for df, ticker in [(tsla_data, 'TSLA'), (bnd_data, 'BND'), (spy_data, 'SPY')]:\n",
    "    returns = df['Daily_Return'].dropna()\n",
    "    var_hist, var_param = calculate_var(returns, confidence_level=0.05)\n",
    "    \n",
    "    print(f\"\\n{ticker}:\")\n",
    "    print(f\"  Historical VaR (5%): {var_hist*100:.4f}%\")\n",
    "    print(f\"  Parametric VaR (5%): {var_param*100:.4f}%\")\n",
    "    print(f\"  Interpretation: We expect losses to exceed {abs(var_hist*100):.2f}% on 5% of trading days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Sharpe Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sharpe_ratio(returns, risk_free_rate=0.02, periods_per_year=252):\n",
    "    \"\"\"\n",
    "    Calculate Sharpe Ratio: (Mean Return - Risk-Free Rate) / Std Dev of Returns\n",
    "    Annualized Sharpe Ratio\n",
    "    \"\"\"\n",
    "    mean_return = returns.mean() * periods_per_year  # Annualized\n",
    "    std_return = returns.std() * np.sqrt(periods_per_year)  # Annualized\n",
    "    \n",
    "    # Annualized risk-free rate\n",
    "    rf_annual = risk_free_rate\n",
    "    \n",
    "    sharpe_ratio = (mean_return - rf_annual) / std_return\n",
    "    \n",
    "    return sharpe_ratio, mean_return, std_return\n",
    "\n",
    "print(\"Sharpe Ratio (Risk-Adjusted Returns)\")\n",
    "print(\"=\"*70)\n",
    "print(\"Assumes 2% annual risk-free rate\")\n",
    "print(\"Higher Sharpe Ratio = Better risk-adjusted returns\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for df, ticker in [(tsla_data, 'TSLA'), (bnd_data, 'BND'), (spy_data, 'SPY')]:\n",
    "    returns = df['Daily_Return'].dropna()\n",
    "    sharpe, ann_return, ann_vol = calculate_sharpe_ratio(returns)\n",
    "    \n",
    "    print(f\"\\n{ticker}:\")\n",
    "    print(f\"  Annualized Return: {ann_return*100:.2f}%\")\n",
    "    print(f\"  Annualized Volatility: {ann_vol*100:.2f}%\")\n",
    "    print(f\"  Sharpe Ratio: {sharpe:.4f}\")\n",
    "    \n",
    "    if sharpe > 1:\n",
    "        interpretation = \"Excellent\"\n",
    "    elif sharpe > 0.5:\n",
    "        interpretation = \"Good\"\n",
    "    elif sharpe > 0:\n",
    "        interpretation = \"Acceptable\"\n",
    "    else:\n",
    "        interpretation = \"Poor\"\n",
    "    print(f\"  Interpretation: {interpretation} risk-adjusted returns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary of Key Insights\n",
    "\n",
    "### Data Quality\n",
    "- All data successfully extracted from YFinance\n",
    "- No missing values detected\n",
    "- Data types properly formatted\n",
    "- Date range: January 1, 2015 to January 15, 2026\n",
    "\n",
    "### Key Patterns Identified\n",
    "1. **TSLA**: High volatility, significant growth, extreme returns\n",
    "2. **BND**: Low volatility, stable prices, consistent returns\n",
    "3. **SPY**: Moderate volatility, steady growth, market-representative returns\n",
    "\n",
    "### Stationarity\n",
    "- Closing prices are non-stationary (require differencing for ARIMA)\n",
    "- Daily returns are stationary (suitable for ARIMA with d=0 or d=1)\n",
    "\n",
    "### Risk Profile\n",
    "- TSLA: Highest risk (highest VaR, variable Sharpe ratio)\n",
    "- BND: Lowest risk (lowest VaR, stable returns)\n",
    "- SPY: Moderate risk (balanced risk-return profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data for Task 2\n",
    "tsla_data.to_csv('../data/processed/TSLA_processed.csv')\n",
    "bnd_data.to_csv('../data/processed/BND_processed.csv')\n",
    "spy_data.to_csv('../data/processed/SPY_processed.csv')\n",
    "combined_data.to_csv('../data/processed/combined_data.csv')\n",
    "\n",
    "print(\"Processed data saved to ../data/processed/\")\n",
    "print(\"Files saved:\")\n",
    "print(\"  - TSLA_processed.csv\")\n",
    "print(\"  - BND_processed.csv\")\n",
    "print(\"  - SPY_processed.csv\")\n",
    "print(\"  - combined_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
